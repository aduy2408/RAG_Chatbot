{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APEC 2025 Korea Website Scraping\n",
    "\n",
    "This notebook will scrape content from the APEC 2025 Korea website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url, timeout=10):\n",
    "\n",
    "    print(f\"Fetching: {url}\")\n",
    "    response = session.get(url, timeout=timeout)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    if response.encoding == 'ISO-8859-1':\n",
    "        response.encoding = response.apparent_encoding\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://apec2025.kr/?menuno=1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://apec2025.kr/?menuno=1\"\n",
    "soup = get_soup(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Main > APEC 2025 KOREA\n",
      "\n",
      "Page structure:\n",
      "Links: 82\n",
      "Images: 26\n",
      "Paragraphs: 21\n",
      "Headings: 15\n"
     ]
    }
   ],
   "source": [
    "if soup:\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "    print(f\"Title: {title}\")\n",
    "    \n",
    "    print(f\"\\nPage structure:\")\n",
    "    print(f\"Links: {len(soup.find_all('a'))}\")\n",
    "    print(f\"Images: {len(soup.find_all('img'))}\")\n",
    "    print(f\"Paragraphs: {len(soup.find_all('p'))}\")\n",
    "    print(f\"Headings: {len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82 links:\n",
      "- Go to main content... -> https://apec2025.kr#contents\n",
      "- youtube... -> https://www.youtube.com/@APEC2025KOREA/videos\n",
      "- instagram... -> https://www.instagram.com/apec2025korea/\n",
      "- facebook... -> https://www.facebook.com/apec2025korea1\n",
      "- flickr... -> https://www.flickr.com/photos/apec2025\n",
      "- KOR... -> https://apec2025.kr/kor/\n",
      "- About APEC 2025 KOREA... -> https://apec2025.kr/\n",
      "- Mobile menu... -> https://apec2025.kr#none\n",
      "- About APEC 2025 KOREA... -> https://apec2025.kr?menuno=2\n",
      "- APEC... -> https://apec2025.kr?menuno=89\n",
      "- APEC 2025 KOREA... -> https://apec2025.kr?menuno=90\n",
      "- Meetings... -> https://apec2025.kr?menuno=93\n",
      "- Side Event... -> https://apec2025.kr?menuno=94\n",
      "- Media... -> https://apec2025.kr?menuno=14\n",
      "- Notices... -> https://apec2025.kr?menuno=15\n",
      "- Press Releases... -> https://apec2025.kr?menuno=16\n",
      "- Resources... -> https://apec2025.kr?menuno=17\n",
      "- Social Media... -> https://apec2025.kr?menuno=98\n",
      "- Partners... -> https://apec2025.kr?menuno=100\n",
      "- Sponsorship... -> https://apec2025.kr?menuno=100\n",
      "- Visit Korea... -> https://apec2025.kr?menuno=18\n",
      "- K-story... -> https://apec2025.kr?menuno=19\n",
      "- Gyeongju... -> https://apec2025.kr?menuno=102\n",
      "- Jeju... -> https://apec2025.kr?menuno=103\n",
      "- Incheon... -> https://apec2025.kr?menuno=104\n",
      "- Busan... -> https://apec2025.kr?menuno=106\n",
      "- Seoul... -> https://apec2025.kr?menuno=24\n",
      "- Asia-Pacific Economic Cooperation... -> https://www.apec.org/\n",
      "- SITE MAP... -> https://apec2025.kr#pop-up01\n",
      "- 유튜브... -> https://www.youtube.com/@APEC2025KOREA/videos\n",
      "- 인스타그램... -> https://www.instagram.com/apec2025korea/\n",
      "- 페이스북... -> https://www.facebook.com/apec2025korea1\n",
      "- 플리커... -> https://www.flickr.com/photos/apec2025\n",
      "- ... -> https://www.youtube.com/watch?v=NIjIqFhxZR4\n",
      "- ... -> https://www.instagram.com/reel/DLWoD5ySyLd/?utm_source=ig_web_copy_link&igsh=MzRlODBiNWFlZA%3D%3D\n",
      "- ... -> https://www.facebook.com/story.php?story_fbid=122123911346851512&id=61575545381061&rdid=ZBh22EyXDEQvICme\n",
      "- ... -> https://www.flickr.com/photos/apec2025/\n",
      "- View More... -> javascript:;\n",
      "- ... -> https://apec2025.kr/?menuno=93\n",
      "- View More... -> javascript:;\n",
      "- ... -> https://apec2025.kr/?menuno=93\n",
      "- View More... -> javascript:;\n",
      "- ... -> https://apec2025.kr/?menuno=93\n",
      "- View More... -> javascript:;\n",
      "- ... -> https://apec2025.kr/?menuno=93\n",
      "- View More... -> javascript:;\n",
      "- ... -> https://apec2025.kr/?menuno=93\n",
      "- view more... -> https://apec2025.kr?menuno=90\n",
      "- 재생버튼... -> https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "- 재생버튼... -> https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "- 6th Meeting of Korea APEC 2025 Organizing Committe... -> https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "- Comprehensive review of preparations conducted ahe... -> https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "- Korea strengthens cooperation with Asia-Pacific ec... -> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDEiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "- 2025 APEC Ministers Responsible for Trade Joint St... -> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDAiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "- Global trade leaders gather on Jeju Island for 202... -> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzkiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "- APEC Second Senior Officials’ Meeting (SOM2) held ... -> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzYiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "- View more +... -> https://apec2025.kr?menuno=16\n",
      "- About  APEC 2025 Korea Logo... -> https://apec2025.kr/\n",
      "- youtube... -> https://www.youtube.com/@APEC2025KOREA/videos\n",
      "- instargram... -> https://www.instagram.com/apec2025korea/\n",
      "- facebook... -> https://www.facebook.com/apec2025korea1\n",
      "- flickr... -> https://www.flickr.com/photos/apec2025\n",
      "- About APEC 2025 KOREA... -> https://apec2025.kr?menuno=2\n",
      "- APEC... -> https://apec2025.kr?menuno=89\n",
      "- APEC 2025 KOREA... -> https://apec2025.kr?menuno=90\n",
      "- Meetings... -> https://apec2025.kr?menuno=93\n",
      "- Side Event... -> https://apec2025.kr?menuno=94\n",
      "- Media... -> https://apec2025.kr?menuno=14\n",
      "- Notices... -> https://apec2025.kr?menuno=15\n",
      "- Press Releases... -> https://apec2025.kr?menuno=16\n",
      "- Resources... -> https://apec2025.kr?menuno=17\n",
      "- Social Media... -> https://apec2025.kr?menuno=98\n",
      "- Partners... -> https://apec2025.kr?menuno=100\n",
      "- Sponsorship... -> https://apec2025.kr?menuno=100\n",
      "- Visit Korea... -> https://apec2025.kr?menuno=18\n",
      "- K-Story... -> https://apec2025.kr?menuno=19\n",
      "- Gyeongju... -> https://apec2025.kr?menuno=102\n",
      "- Jeju... -> https://apec2025.kr?menuno=103\n",
      "- Incheon... -> https://apec2025.kr?menuno=104\n",
      "- Busan... -> https://apec2025.kr?menuno=106\n",
      "- Seoul... -> https://apec2025.kr?menuno=24\n"
     ]
    }
   ],
   "source": [
    "if soup:\n",
    "    links = soup.find_all('a', href=True)\n",
    "    print(f\"Found {len(links)} links:\")\n",
    "    \n",
    "    base_url = \"https://apec2025.kr\"\n",
    "    all_links = []\n",
    "    \n",
    "    for link in links[:-1]:  \n",
    "        href = link['href']\n",
    "        text = link.get_text(strip=True)\n",
    "        \n",
    "        full_url = urljoin(base_url, href)\n",
    "        all_links.append({'url': full_url, 'text': text})\n",
    "        \n",
    "        print(f\"- {text[:50]}... -> {full_url}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(table):\n",
    "    \"\"\"Convert table to structured text format\"\"\"\n",
    "    table_text = [\"\\n[TABLE START]\\n\"]\n",
    "    \n",
    "    headers = table.find_all('th')\n",
    "    if headers:\n",
    "        header_texts = [th.get_text(strip=True) for th in headers]\n",
    "        table_text.append(\"HEADERS: \" + \" | \".join(header_texts) + \"\\n\")\n",
    "        table_text.append(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    for i, row in enumerate(rows, 1):\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if cells:\n",
    "            cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "            cell_texts = [text for text in cell_texts if text]\n",
    "            if cell_texts:\n",
    "                table_text.append(f\"ROW {i}: \" + \" | \".join(cell_texts) + \"\\n\")\n",
    "    \n",
    "    table_text.append(\"[TABLE END]\\n\\n\")\n",
    "    return ''.join(table_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_content_dedup(soup):\n",
    "    content_div = soup.find('div', class_='contents')\n",
    "\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in content_div([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"noscript\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "\n",
    "    # Track seen content to avoid duplicates\n",
    "    seen_content = set()\n",
    "    structured_text = []\n",
    "    processed_elements = set()\n",
    "    \n",
    "    # Process all elements in order\n",
    "    for element in content_div.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'table']):\n",
    "        if any(element in processed for processed in processed_elements):\n",
    "            continue\n",
    "            \n",
    "        if element.name == 'table':\n",
    "            table_content = process_table(element)\n",
    "            if table_content not in seen_content:\n",
    "                structured_text.append(table_content)\n",
    "                seen_content.add(table_content)\n",
    "            processed_elements.add(element)\n",
    "            for desc in element.find_all():\n",
    "                processed_elements.add(desc)\n",
    "            continue\n",
    "        \n",
    "        text = element.get_text(strip=True)\n",
    "        if not text or len(text) < 3:  # Skip very short text\n",
    "            continue\n",
    "            \n",
    "        normalized_text = ' '.join(text.split()).lower()\n",
    "        \n",
    "        if element.name == 'h2':\n",
    "            formatted_text = f\"\\n\\n=== {text.upper()} ===\\n\"\n",
    "            if normalized_text not in seen_content:\n",
    "                structured_text.append(formatted_text)\n",
    "                seen_content.add(normalized_text)\n",
    "        elif element.name == 'h3':\n",
    "            formatted_text = f\"\\n--- {text} ---\\n\"\n",
    "            if normalized_text not in seen_content:\n",
    "                structured_text.append(formatted_text)\n",
    "                seen_content.add(normalized_text)\n",
    "        elif element.name in ['h1', 'h4', 'h5', 'h6']:\n",
    "            formatted_text = f\"\\n{text}\\n\"\n",
    "            if normalized_text not in seen_content:\n",
    "                structured_text.append(formatted_text)\n",
    "                seen_content.add(normalized_text)\n",
    "        else:\n",
    "            if not element.find_parent('table') and normalized_text not in seen_content:\n",
    "                structured_text.append(text + \" \")\n",
    "                seen_content.add(normalized_text)\n",
    "    \n",
    "    # Join and clean up\n",
    "    final_text = ''.join(structured_text)\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    lines = final_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = ' '.join(line.split())\n",
    "        cleaned_lines.append(cleaned_line)\n",
    "    \n",
    "    # Remove excessive empty lines\n",
    "    result_lines = []\n",
    "    empty_count = 0\n",
    "    for line in cleaned_lines:\n",
    "        if line.strip() == '':\n",
    "            empty_count += 1\n",
    "            if empty_count <= 2:\n",
    "                result_lines.append(line)\n",
    "        else:\n",
    "            empty_count = 0\n",
    "            result_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(result_lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url, max_retries=3):\n",
    "    try:\n",
    "        soup = get_soup(url)\n",
    "\n",
    "        title = soup.title.string if soup.title else \"No title\"\n",
    "        \n",
    "        content = extract_text_content_dedup(soup)\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title.strip(),\n",
    "            'content': content,\n",
    "            'word_count': len(content.split()),\n",
    "            'status': 'success'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 unique APEC 2025 Korea pages to scrape:\n",
      "1. About APEC 2025 KOREA-> https://apec2025.kr?menuno=2\n",
      "2. APEC-> https://apec2025.kr?menuno=89\n",
      "3. APEC 2025 KOREA-> https://apec2025.kr?menuno=90\n",
      "4. Meetings-> https://apec2025.kr?menuno=93\n",
      "5. Side Event-> https://apec2025.kr?menuno=94\n",
      "6. Media-> https://apec2025.kr?menuno=14\n",
      "7. Notices-> https://apec2025.kr?menuno=15\n",
      "8. Press Releases-> https://apec2025.kr?menuno=16\n",
      "9. Resources-> https://apec2025.kr?menuno=17\n",
      "10. Social Media-> https://apec2025.kr?menuno=98\n",
      "11. Partners-> https://apec2025.kr?menuno=100\n",
      "12. Visit Korea-> https://apec2025.kr?menuno=18\n",
      "13. K-story-> https://apec2025.kr?menuno=19\n",
      "14. Gyeongju-> https://apec2025.kr?menuno=102\n",
      "15. Jeju-> https://apec2025.kr?menuno=103\n",
      "16. Incheon-> https://apec2025.kr?menuno=104\n",
      "17. Busan-> https://apec2025.kr?menuno=106\n",
      "18. Seoul-> https://apec2025.kr?menuno=24\n",
      "19. -> https://apec2025.kr/?menuno=93\n",
      "20. 재생버튼-> https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "21. Korea strengthens cooperation with Asia-Pacific economies to-> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDEiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "22. 2025 APEC Ministers Responsible for Trade Joint Statement202-> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDAiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "23. Global trade leaders gather on Jeju Island for 2025 APEC MRT-> https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzkiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n"
     ]
    }
   ],
   "source": [
    "# Filter links to only include APEC 2025 Korea pages \n",
    "apec_links = []\n",
    "for link_data in all_links:\n",
    "    url = link_data['url']\n",
    "    # Only include apec2025.kr links and exclude javascript links\n",
    "    if 'apec2025.kr' in url and not url.startswith('javascript:') and '?menuno=' in url:\n",
    "        apec_links.append(link_data)\n",
    "\n",
    "# Remove duplicates\n",
    "unique_urls = set()\n",
    "unique_apec_links = []\n",
    "for link in apec_links:\n",
    "    if link['url'] not in unique_urls:\n",
    "        unique_urls.add(link['url'])\n",
    "        unique_apec_links.append(link)\n",
    "\n",
    "print(f\"Found {len(unique_apec_links)} unique APEC 2025 Korea pages to scrape:\")\n",
    "for i, link in enumerate(unique_apec_links[:-1], 1):\n",
    "    print(f\"{i}. {link['text'][:60]}-> {link['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/24] Scraping: About APEC 2025 KOREA...\n",
      "Fetching: https://apec2025.kr?menuno=2\n",
      "Success: 843 words\n",
      "\n",
      "[2/24] Scraping: APEC...\n",
      "Fetching: https://apec2025.kr?menuno=89\n",
      "Success: 843 words\n",
      "\n",
      "[3/24] Scraping: APEC 2025 KOREA...\n",
      "Fetching: https://apec2025.kr?menuno=90\n",
      "Success: 937 words\n",
      "\n",
      "[4/24] Scraping: Meetings...\n",
      "Fetching: https://apec2025.kr?menuno=93\n",
      "Success: 725 words\n",
      "\n",
      "[5/24] Scraping: Side Event...\n",
      "Fetching: https://apec2025.kr?menuno=94\n",
      "Success: 227 words\n",
      "\n",
      "[6/24] Scraping: Media...\n",
      "Fetching: https://apec2025.kr?menuno=14\n",
      "Success: 98 words\n",
      "\n",
      "[7/24] Scraping: Notices...\n",
      "Fetching: https://apec2025.kr?menuno=15\n",
      "Success: 98 words\n",
      "\n",
      "[8/24] Scraping: Press Releases...\n",
      "Fetching: https://apec2025.kr?menuno=16\n",
      "Success: 14392 words\n",
      "\n",
      "[9/24] Scraping: Resources...\n",
      "Fetching: https://apec2025.kr?menuno=17\n",
      "Success: 91 words\n",
      "\n",
      "[10/24] Scraping: Social Media...\n",
      "Fetching: https://apec2025.kr?menuno=98\n",
      "Success: 51 words\n",
      "\n",
      "[11/24] Scraping: Partners...\n",
      "Fetching: https://apec2025.kr?menuno=100\n",
      "Success: 12 words\n",
      "\n",
      "[12/24] Scraping: Visit Korea...\n",
      "Fetching: https://apec2025.kr?menuno=18\n",
      "Success: 82 words\n",
      "\n",
      "[13/24] Scraping: K-story...\n",
      "Fetching: https://apec2025.kr?menuno=19\n",
      "Success: 82 words\n",
      "\n",
      "[14/24] Scraping: Gyeongju...\n",
      "Fetching: https://apec2025.kr?menuno=102\n",
      "Success: 202 words\n",
      "\n",
      "[15/24] Scraping: Jeju...\n",
      "Fetching: https://apec2025.kr?menuno=103\n",
      "Success: 189 words\n",
      "\n",
      "[16/24] Scraping: Incheon...\n",
      "Fetching: https://apec2025.kr?menuno=104\n",
      "Success: 187 words\n",
      "\n",
      "[17/24] Scraping: Busan...\n",
      "Fetching: https://apec2025.kr?menuno=106\n",
      "Success: 214 words\n",
      "\n",
      "[18/24] Scraping: Seoul...\n",
      "Fetching: https://apec2025.kr?menuno=24\n",
      "Success: 228 words\n",
      "\n",
      "[19/24] Scraping: ...\n",
      "Fetching: https://apec2025.kr/?menuno=93\n",
      "Success: 725 words\n",
      "\n",
      "[20/24] Scraping: 재생버튼...\n",
      "Fetching: https://apec2025.kr/?menuno=16&act=view&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzQiIHNpdGVubz0iMiI-PC9jYWxsPg==\n",
      "Success: 2557 words\n",
      "\n",
      "[21/24] Scraping: Korea strengthens cooperation with Asia-Pacific ec...\n",
      "Fetching: https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDEiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "Success: 1904 words\n",
      "\n",
      "[22/24] Scraping: 2025 APEC Ministers Responsible for Trade Joint St...\n",
      "Fetching: https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iNDAiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "Success: 7231 words\n",
      "\n",
      "[23/24] Scraping: Global trade leaders gather on Jeju Island for 202...\n",
      "Fetching: https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzkiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "Success: 1395 words\n",
      "\n",
      "[24/24] Scraping: APEC Second Senior Officials’ Meeting (SOM2) held ...\n",
      "Fetching: https://apec2025.kr/?menuno=16&ztag=rO0ABXQAUTxjYWxsIHR5cGU9ImJvYXJkIiBubz0iNyIgc2tpbj0icGhvdG90aHVtYl9zdWJtaXQyIiBiYnNubz0iMzYiIHNpdGVubz0iMiI-PC9jYWxsPg%3D%3D&act=view\n",
      "Success: 2872 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scraped_data = []\n",
    "total_pages = len(unique_apec_links)\n",
    "\n",
    "\n",
    "for i, link_data in enumerate(unique_apec_links, 1):\n",
    "    url = link_data['url']\n",
    "    print(f\"[{i}/{total_pages}] Scraping: {link_data['text'][:50]}...\")\n",
    "    \n",
    "    page_data = scrape_page_content(url)\n",
    "    page_data['link_text'] = link_data['text']\n",
    "    scraped_data.append(page_data)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Show progress\n",
    "    if page_data['status'] == 'success':\n",
    "        print(f\"Success: {page_data['word_count']} words\")\n",
    "    else:\n",
    "        print(f\"{page_data['status'].title()}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to: apec2025_scraped_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"apec2025_scraped_data.json\"\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(scraped_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Scraped data saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('scraped_pages', exist_ok=True)\n",
    "\n",
    "for i, page in enumerate(successful_scrapes, 1):\n",
    "    safe_title = \"\".join(c for c in page['title'] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "    safe_title = safe_title[:50]  # Limit length\n",
    "    filename = f\"scraped_pages/{i:02d}_{safe_title}.txt\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Title: {page['title']}\\n\")\n",
    "        f.write(f\"URL: {page['url']}\\n\")\n",
    "        f.write(f\"Word Count: {page['word_count']}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(page['content'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
